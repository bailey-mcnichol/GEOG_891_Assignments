---
title: 'Lab 1: Intro to Attribute and Spatial Analysis in R'
author: "Bailey McNichol"
date: "9/17/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r include=FALSE, echo=TRUE}
library(tidyverse)
library(ggplot2) #technically included in tidyverse
library(sf)
library(sp) #just in case
```

##### Load and examine the data
```{r get my data, include=FALSE, echo=TRUE}

p.counties <- "../data/CBW/County_Boundaries.shp"
p.stations <- "../data/CBW/Non-Tidal_Water_Quality_Monitoring_Stations_in_the_Chesapeake_Bay.shp"

d.counties <- sf::read_sf(p.counties)
d.stations <- sf::read_sf(p.stations)

#glimpse(d.counties)
#glimpse(d.stations)

```


##### Select attributes that we specify
```{r selecting stuff, message=TRUE, echo=TRUE}
d.counties %>% dplyr::select(GEOID10, ALAND10) %>% head() 

```

##### Remove unwanted attributes with "-"
```{r noname, message=TRUE, echo=TRUE}
d.counties %>% dplyr::select(-NAME10) %>% head() 

```


##### Specify ranges that we want to keep (or not):
```{r keeprange, echo=TRUE, message=TRUE}
d.counties %>% dplyr::select(GEOID10:CLASSFP10) %>% head() 

d.counties %>% dplyr::select(-(GEOID10:CLASSFP10)) %>% head() 

d.counties %>% dplyr::select(starts_with("C"))
```


##### Group data - create a new attribute that calculates the land area of all counties in each state.
```{r grpuex, echo=TRUE, message=TRUE}
d.counties %>% group_by(STATEFP10) %>% mutate(stateLandArea = sum(ALAND10))
```


##### Convert sf data frame to a tibble, then remove the geometry before performing the `group_by` and `summarise` functions.
```{r groupSummarise, echo=TRUE, message=TRUE}
d.counties %>% 
  # this line converts the data because of wonky geometry
  as_tibble() %>% dplyr::select(-geometry) %>% 
  group_by(STATEFP10) %>% 
  summarise(stateLandArea = sum(ALAND10))
```

##### Use grouping in a plot
```{r plots, echo=TRUE, message=TRUE}
d.counties %>% 
  ggplot(., aes(x = as.factor(STATEFP10), y = ALAND10)) +
  geom_boxplot(aes(fill = STATEFP10))
```

```{r plots2, echo=TRUE, message=TRUE}
d.counties %>% 
  ggplot(., aes(x = ALAND10)) +
  geom_histogram(aes(fill = STATEFP10)) +
  labs(title = "not the most useful plot, but you get the idea")
```


##### Examine coordinate reference system (CRS) for each file
```{r crs, echo=TRUE, message=TRUE}
d.counties %>% sf::st_crs()
d.stations %>% sf::st_crs()
```

##### Formally check CRS are the same
```{r crscomp, echo=TRUE, message=TRUE}
d.counties %>% sf::st_crs() == d.stations %>% sf::st_crs()
```

##### Subset data to only include counties in Delaware

```{r mysubset, echo=TRUE, message=TRUE}
del.counties <- d.counties %>% dplyr::filter(STATEFP10 == 10)
```

##### Perform a *spatial intersection* to find all of the monitoring stations within our Delaware subset

```{r intersect, echo=TRUE, message=TRUE}
del.stations <- sf::st_intersection(d.stations, del.counties)
```

##### Examine data and plot
```{r mypoints, echo=TRUE, message=TRUE}
glimpse(del.stations)
#plot(del.stations)
```

##### Calculation of the area of each county in Delaware using sf function
```{r areacalc, echo=TRUE, message=TRUE}
del.counties %>% st_area() 
```


## Lab 1 Tasks

### Task 1: Basic data manipulation

#### 1.1 For each county, calculate its land area as percentage of the total area (land + water) for that state.

I created a new tibble (d.counties.2) that groups data by state and then calculates the percent of state land area for each county (new variable = countyLandPercent). The column 'countyLandPercent' includes those values, and the last line is an easy check to make sure the percentages don't exceed 100% in each state.
```{r include=TRUE, echo=TRUE}

d.counties.2 <- d.counties %>% 
  # Group the data by state
        group_by(STATEFP10) %>% 
  # Calculate the percent of state land area for each county (new variable = countyLandPercent)
        mutate(countyLandPercent = ((ALAND10+AWATER10)/sum(ALAND10+AWATER10))*100)

# View values of the new variable - each county is a percent of its state 
d.counties.2$countyLandPercent

# Check on the operation to make sure we haven't made a mistake - new variable sums to 700% (100% for each of the 7 states included in the Chesapeake Bay Watershed)
sum(d.counties.2$countyLandPercent) 
```


#### 1.2 For each state, find the county that has the largest proportion of its land as water (water area / total area)

I first removed the weird geometry, then grouped data by county, calculated the proportion of water per county, then grouped by state to find the county with the largest proportion within each state. The final output lists county in each state with the largest proportion of land as water (proportions listed in 'countyWaterProp').
```{r include=TRUE, echo=TRUE}
       d.counties %>% 
# First, remove weird geometry from the tibble
        as_tibble() %>% dplyr::select(-geometry) %>%
# Group data by county
        group_by(NAME10) %>% 
# Then, calculate the proportion of area occupied by water for each county
        mutate(totalArea = ALAND10 + AWATER10,
          countyWaterProp = AWATER10/totalArea)  %>%
# Then, group data by state
        group_by(STATEFP10) %>% 
# Filter to only include the max county per state
        slice(which.max(countyWaterProp)) %>%
# Select to only include some columns in output
        dplyr::select(c(STATEFP10,NAME10,AWATER10,totalArea,countyWaterProp))

```


#### 1.3 Count the number of counties in each state

I first removed the weird geometry, then grouped the data by state, and then counted the number of rows per state with n() to get the number of counties in each state. The output lists these counts.
```{r include=TRUE, echo=TRUE}

# Each row represents a county within a state, so we can remove the weird geometry, group the data by state, and then count the number of rows per state with n() to get the number of counties in each state
d.counties %>% 
        as_tibble() %>% dplyr::select(-geometry) %>%
        group_by(STATEFP10) %>% 
        summarise(NumberCounties = n())

```


#### 1.4 Which station has the shortest name (STATION_NA) in the study area?

I examined the lengths of character strings to determine which station name was shortest. I found that the shortest name, ABRAM CREEK AT OAKMONT, WV, had 26 characters.
```{r include=TRUE, echo=TRUE}
# Determine the minimum character string length (name) for Station Name in the d.stations dataset - 26 characters
min(str_length(d.stations$STATION_NA))
# Determine which position has that minimum (which index) - 105
which.min(str_length(d.stations$STATION_NA))
# Index the data to determine which station is in the 105th position
d.stations$STATION_NA[105]

```


### Task 2: Plotting attribute data
Label your axes properly and give each plot a title

#### 2.1 Make a scatterplot showing the relationship between land area and water area for each county. Color each point using the state variable
```{r include=TRUE, echo=TRUE}
ggplot(data = d.counties) + 
    geom_point(mapping = aes(x = ALAND10, y =  AWATER10, 
    color = STATEFP10)) +
    theme_bw() +
    labs(x = "Land Area (m2)", y = "Water Area (m2)", 
    title = "Relationship of County Land vs. Water Area (m2) \n in Chesapeake Bay Watershed, by State", color = "State FIPS Code") 
```


#### 2.2 Make a histogram of drainage area (Drainage_A) for all monitoring stations
```{r include=TRUE, echo=TRUE}
ggplot(data = d.stations) + 
    geom_histogram(mapping = aes(x = Drainage_A), fill = "orchid") +
    theme_bw() +
    labs(x = "Drainage Area (sq. mi.)", y = "Count", 
    title = "Distribution of Drainage Areas (sq. mi.) for Monitoring Stations \n across the Chesapeake Bay Watershed") 
```


#### 2.3 Make a similar histogram, this time of drainage area (Drainage_A) for all monitoring stations. Color each point using the state variable.

To accomplish this, I created a state variable (STATE) within d.stations by extracting the two-letter state abbreviation from the Station Names. I then incorporated this into the histogram as a fill variable to color code by state.
```{r include=TRUE, echo=TRUE}
# First, we need to make a new variable for state - we can do this using the last two characters of the USGS Station Name (Station_NA)
n_last <- 2                                
d.stations$STATE <- substr(d.stations$STATION_NA, 
            nchar(d.stations$STATION_NA) - n_last + 1, nchar(d.stations$STATION_NA)) 
# Now, we can make the histogram, coloring stations by state
ggplot(data = d.stations) + 
    geom_histogram(mapping = aes(x = Drainage_A, fill = STATE)) +
    theme_bw() +
    labs(x = "Drainage Area (sq. mi.)", y = "Count", fill = "State",
    title = "Distribution of Drainage Areas (sq. mi.) for Monitoring Stations \n across the Chesapeake Bay Watershed, by State") 
```


### Task 3: Write a function

#### 3.1 Write a function that does the following:
#### A. Accepts a vector of arbitrary numbers, calculates the mean, median, maximum, and minimum of the vector
#### B. Sorts the vector
#### C. Returns a list of those values from A and the sorted vector from B
#### D. The function should only work with numeric values and print an error message if any other data type are found

I have written annotations throughout my function explaining what the various components are accomplishing.
```{r include=TRUE, echo=TRUE}

my.fxn <- function(x) {
  # Command to abort running function if vector includes non-numeric values  
  if(is.numeric(x) == "FALSE") {
        print("Value(s) in the supplied vector are not numeric")
          } else {
    # If data is numeric, then calculate the mean, median, max, and min  
      mean <- mean(x)
      median <- median(x)
      maximum <- max(x)
      minimum <- min(x)
    # Also, sort the vector in ascending order
      sorted_vector <- sort(x)
    # Finally, give me an output that returns these quantities
      output = list(mean = mean, 
                    median = median, 
                    maximum = maximum, 
                    minimum = minimum, 
                    sorted_vector = sorted_vector)
      return(output)
      }
  }

```

#### Test it with the following vectors
#### `c(1, 0, -1), c(10, 100, 1000), c(.1, .001, 1e8), c("a", "b", "c")`

See the results of running the test vectors through my function below.
```{r include=TRUE, echo=TRUE}
# Label the vectors to pass through my function
v1 <- c(1, 0, -1)
v2 <- c(10, 100, 1000)
v3 <- c(.1, .001, 1e8)
v4 <- c("a", "b", "c")

# Test the function
my.fxn(v1)
my.fxn(v2)
my.fxn(v3)
my.fxn(v4)
```


### Task 4: (slightly) more complex spatial analysis. 
...Note, you may need to find supplementary data to help you with these tasks

#### 4.1 Calculate the number of monitoring stations in each state

To accomplish this, I removed the weird geometry, then grouped by the STATE variable that I made above (when creating the second histogram with groups), and then counted the number of stations (each is a row) with summarize(). The output shows the state and these counts.
```{r include=TRUE, echo=TRUE}

d.stations %>% 
        # Remove geometry to avoid weird errors
        as_tibble() %>% dplyr::select(-geometry) %>%
        # Group by the State variable in d.stations defined above when making histogram
        group_by(STATE) %>% 
        # Count the number of stations per state
        summarise(NumberStations = n())

```

#### 4.2 Calculate the average size of counties in New York (that are also in this study area)

First, I removed the weird geometry, then filtered the data to create a subset only including counties in New York, and then I calculated the mean county area for New York counties using summarize(), which is displayed in the output below.
```{r include=TRUE, echo=TRUE}
        d.counties %>% 
  # Remove weird geometry from data
              as_tibble() %>% dplyr::select(-geometry) %>%
  # Filter to only include counties in New York using FIPS code
              dplyr::filter(STATEFP10 == 36) %>%
  # calculate the average county area in m^2 (need to include both land and water area) with summarize()
              summarize(AverageSize = mean(ALAND10 + AWATER10))
  
```

#### 4.3 Calculate which state has monitoring stations with the greatest average drainage area (Drainage_A)

I removed the weird geometry from d.stations, grouped by the state variable (STATE) that I previously defined, calculated the mean drainage area by state with summarize, and then used the which.max() function inside slice() to determine which state has monitoring stations with the greatest mean drainage area. We see in the output that this state is Pennsylvania with a mean area of 3549.196 square miles.
```{r include=TRUE, echo=TRUE}
 
  d.stations %>% 
  # Remove geometry to avoid weird errors
        as_tibble() %>% dplyr::select(-geometry) %>%
  # Group by the State variable in d.stations defined above when making histogram
        group_by(STATE) %>% 
  # Calculate the average drainage area across monitoring stations within each state
        summarise(MeanDrainageArea = mean(Drainage_A)) %>%
  # Use slice() and which.max() to determine which state has the greatest average drainage area (in square miles)
        slice(which.max(MeanDrainageArea))

```


## Questions

#### 1. In using the intersection functions, are the following two statements equivalent? If not, explain how. Be sure to think about BOTH the spatial data structures AND the attribute data. Would your answer be different if we were using different types of data?

I have provided some description below of the differences between these two intersection statements. Overall, the answer that each statement gives you (i.e., the data that is merged) is the same, but the order in which the information is compiled is reversed. The first statement gives station information merged with the relevant county data, whereas the second statement gives county information merged with the relevant station data. The attribute data produced by both statements with the intersection function is analagous, as these variables are assumed to be constant spatially across geometries (point geometry with same number of features and fields). If the types of data were not consistent, this intersection would produce different results depending on the order of the elements in the statement (thus affecting the resulting geometry and structure).
```{r}
# This statement finds all of the monitoring stations in counties in Delaware, showing station information first, and then merging the data about the counties (so we see county Object ID column is renamed to ObjectID.1 to avoid identical names)
sf::st_intersection(d.stations, del.counties)

# This statement also finds all of the monitoring stations in Delaware counties, but sorts the information differently; first it displays the county information (here we see that the ObjectID column refers to the county IDs), and then the monitoring station data (so the renamed ObjectID.1 column is now the station IDs)
sf::st_intersection(del.counties, d.stations)
```



#### 2. What did you find challenging in this lab? What was new?

Because I am not used to coding in the tidyverse(), I had a bit of a learning curve with the syntax and pipes, although it really is not that hard. But I really enjoyed it and am kicking myself for not just committing to using tidyr() more extensively earlier! It also took me longer than it should have to get the error message to print in the if else statement in my function...I also need to keep practicing my function writing! The main challenge for me is starting to visualize and interpret the geometries and overall structure of spatial data. I think I'll be able to continue connecting the dots as we begin more visualizations of these features.

#### 3. What types of activities would you like to see in labs this semester?

I would like to learn the best ways to merge large datasets efficiently to perform spatial analyses, e.g., merging coordinate or other spatial reference data with the rest of your data (in my case, plant variables). I would also like to keep having opportunities to practice writing useful, but more complicated functions to iterate through tasks. I know you said we're going to be pretty minimal on using for loops, but they are often useful in my work and they take me *way* too long to code every time... Just more practice and opportunities to add a spatial component to other types of models analyses (e.g., accounting for location/spatial autocorrelation in a model predicting diversity across habitats).
